\documentclass[a4paper,10pt]{jarticle}
\usepackage{amsmath}
\usepackage[legacycolonsymbols]{mathtools}
\usepackage[dvipdfmx]{graphicx}
\usepackage{here}
\usepackage{longtable}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{amsfonts}
\setlength{\textwidth}{165mm}
\setlength{\marginparwidth}{40mm}
\setlength{\textheight}{225mm}
\setlength{\topmargin}{-5mm}
\setlength{\oddsidemargin}{-3.5mm}
\begin{document}
\title{確率論まとめ}
\date{}
\maketitle
このまとめは個人的にまとめたものであり厳密性を考えていない他、間違えがあるかもしれない。
\section{確率空間}
\subsection{標本空間と事象}
\subsubsection{導入}
はじめに言葉を定義する。
ある試行を行ったときの事象を\textcolor{red}{標本点}と呼ぶ。標本点全体を\textcolor{red}{標本空間}と呼ぶ。
ここで標本空間を$\Omega$で示す。
試行を行った時の発生する事象を$A$で表す。$A$は標本空間の部分集合であり、
$A=\Omega$の時$A$を\textcolor{red}{全事象}、$A$が標本点を持たないとき$A$は\textcolor{red}{空事象}と呼ばれる。なお空事象は$\emptyset$で表される。
事象$A$が起きない事象を\textcolor{red}{補事象}と呼び、$A^C$で表される。

さらに2つ以上の事象を考えてみる。2つの事象$A$と$B$において少なくとも一方が起きるという事象を$A$と$B$の\textcolor{red}{和事象}と呼ばれる。これは$A\cup B$で表される。
また$A$、$B$の両方が起こる事象について\textcolor{red}{積事象}と呼び、$A\cap B$で表される。
$A$、$B$が同時に起こりえないことを\textcolor{red}{排反}と呼ぶ。

実例を示してみよう。サイコロを1回振ったときに結果は1,2,3,4,5,6のいずれかである。このときの結果1つ1つが標本点である。
標本空間は
\begin{equation}
    \Omega = \{1,2,3,4,5,6\}\tag{1,1}
\end{equation}
サイコロを振った時に6以下の自然数が出るという事象は$\Omega$のため全事象、7以上の自然数が出るという事象は空事象となる。
サイコロを振った時に奇数が出る事象を$A$とすると、補事象$A^C$は偶数が出る事象となる。
サイコロを振った時に3以下が出る事象を$B$とすると、$A$と$B$の和事象は$\{1,2,3,5\}$,積事象は$\{1,3\}$となる。
サイコロを振った時に5が出る事象を$C$とすると、$A$と$C$は排反である。

\subsubsection{ド・モルガンの法則}
和事象と積事象の補集合に関する法則である。
具体的には次式で表される。
\begin{equation}
    (A\cup B)^C = A^C\cap B^C\tag{1,2}
\end{equation}
\begin{equation}
    (A\cap B)^C = A^C\cup B^C\tag{1,3}
\end{equation}
ここで導入のサイコロの例をもとに実例を挙げてみよう。$A$はサイコロを振った時に奇数が出る事象、$B$はサイコロを振った時に3以下が出る事象である。
\begin{equation}
    (A\cup B)^C = \{1,2,3,5\}^C=\{4,6\}\tag{1,4}
\end{equation}
\begin{equation}
    A^C\cap B^C = \{1,3,5\}^C\cap\{1,2,3\}^C=\{2,4,6\}\cap\{4,5,6\}=\{4,6\}\tag{1,5}
\end{equation}
\begin{equation}
    (A\cap B)^C = \{1,3\}^C = \{2,4,5,6\}\tag{1,6}
\end{equation}
\begin{equation}
    A^C\cup B^C = \{1,3,5\}^C\cap\{1,2,3\}^C=\{2,4,6\}\cap\{4,5,6\} = \{2,4,5,6\}\tag{1,7}
\end{equation}
\subsubsection{結合法則及び分配法則}
確率の事象にも結合法則及び分配法則が適応可能。具体的には次式で表される。
\begin{equation}
    (A\cup B)\cup C = A\cup B \cup C\tag{1,8}
\end{equation}
\begin{equation}
    (A\cap B)\cap C = A\cap B\cap C \tag{1,9}
\end{equation}
\begin{equation}
    (A\cup B)\cap C = (A\cap C)\cup(B\cap C)\tag{1,10}
\end{equation}
\begin{equation}
    (A\cap B)\cup C = (A\cup C)\cap(B\cup C)\tag{1,11}
\end{equation}
\subsection{確率の定義}
確率とは\textcolor{red}{事象の起きやすさを表す量}である。事象$A$が起きる確率を$P(A)$と表す。
具体的には以下に示す\textcolor{red}{確率の公理}を満たす写像$P(A)$を確立と呼ぶ。
\begin{itemize}
    \item 事象Aに対して$P(A)$は実数である。そして$0\leq P(A)\leq 1$
    \item $P(\Omega)=1$
    \item 互いに排反な事象$A_1,\cdots,A_n$に対して、$P(A_1\cup\cdots\cup A_N)=P(A_1)+\cdots+P(A_n)$
\end{itemize}
\subsection{確率の性質}
確率の公理より簡単に導くことができる性質を以下に示す。
\begin{equation}
    P(\emptyset)=0\tag{1,12}
\end{equation}
\begin{equation}
    P(A^C)= 1-P(A)\tag{1,13}
\end{equation}
\begin{equation}
    A\subset B => P(A) \leq P(B)\tag{1,14}
\end{equation}
\subsubsection{加法定理}
さらに確率の公理より次の等式が成立する。
\begin{equation}
    P(A\cup B) = P(A\cap B^C) +P(A\cap B )+ P(A^C\cap B)\tag{1,15}
\end{equation}
\begin{equation}
    P(A) = P(A\cap B^C) +P(A\cap B)\tag{1,16}
\end{equation}
\begin{equation}
    P(A) = P(A^C\cap B) + P(A\cap B)\tag{1,17}
\end{equation}
これら3式を足すことで\textcolor{red}{加法定理}が得られる。
\begin{equation}
    P(A\cup B )= P(A)+P(B)-P(A\cap B)\tag{1,18}
\end{equation}
\subsection{条件付確率}
2つの事象$A$と$B$に対して、事象$A$が起きたという条件の下で事象$B$が起きるといった確率を\textcolor{red}{条件付確率}と呼び$P(B|A)$と表す。
条件付確率は次式で求めることができる。
\begin{equation}
    P(B|A)=\frac{P(A\cap B)}{P(A)}\tag{1,19}
\end{equation}
また次式を変形すると
\begin{equation}
    P(A\cap B)=P(A)P(B|A)\tag{1,20}
\end{equation}
なおこの式を\textcolor{red}{乗法定理}と呼ぶ。

具体例を考えてみる。
袋Xが1つと袋Yが2つ存在し、袋Xには赤玉3個白玉1個、袋Yには赤玉2個白玉1個あるとする。
袋を1つ選びその中から1つ球をとるということを考える。袋Xを引いたときに白玉を引く確率を考える。
袋Xを引く事象を$A$、白玉を引く事象を$B$とする。$P(A)=\frac{1}{3}$であり、$P(A\cap B)=\frac{1}{12}$
よって条件付確率$P(B|A)=\frac{1}{4}$である。

また袋Yを引く事象を$C$、赤玉を引く事象を$D$とすると、$P(C)=\frac{2}{3}$、$P(D|C)=\frac{1}{2}$のため、乗法定理より$P(C\cap D)=\frac{1}{3}$となる。
もしこの具体例がわからない場合は図示して考えてみるとよい。

\subsection{独立性}
2つの事象$A$と$B$に対して、$P(B|A)=P(B)$となるとき、事象$A$と事象$B$は\textcolor{red}{独立}という。
独立である条件は$A$が$B$に依存しないことである。このとき乗法定理は次のように変形できる。
\begin{equation}
    P(A\cap B) = P(A)P(B)\tag{1,21}
\end{equation}

実例としてサイコロを2回振った時の出目の組み合わせを考えてみる。
1回目に振り奇数が出る事象を$A$、2回目に振り奇数が出る事象を$B$とすると、$A$と$B$はお互い依存していない。
よって$P(A\cap B)= P(A)P(B)=\frac{1}{2}\cdot\frac{1}{2}=\frac{1}{4}$となる。
\subsection{ベイズの定理}
事象$A$が事象$B_1,\cdots,B_k$の事象が発生したときのみに起こりうるということを考える。そして$B_1,\cdots,B_k$以外の事象が発生したときに$A$は起こらないこととする。
つまり事象$B_i$が発生し、事象$A$が起こるような条件を考える。
このとき事象Aが起きたときにそれが$B_i$という事象であったという条件付確率を求める。
この時求める確率は$P(B_i|A)$となる。(1,19)式より、
\begin{equation}
    P(B_i|A) = \frac{P(B_i\cap A)}{P(A)}\tag{1,22}
\end{equation}
ここで$A$というのは条件$B,\cdots,B_k$が起こった上で発生するため、
\begin{equation}
    P(A) =\sum^k_{j=1}P(A\cap B_j)=\sum^k_{j=1}P(B_j)P(A|B_j)\tag{1,23}
\end{equation}
よって次式が成立する。
\begin{equation}
    P(B_i|A) = \frac{P(B_i\cap A)}{\sum^k_{j=1}P(B_j)P(A|B_j)}\tag{1,23}
\end{equation}
なおこの式を\textcolor{red}{ベイズの定理}と呼ぶ。

やはり式だけ見てもわかりずらいため実例を出してみる。
袋B$_1$に赤玉3つ白玉1つ、袋B$_2$に赤玉2つ白玉2つ、袋B$_3$に赤玉3つ白玉2つあるとする。
この時袋B$_i$を選ぶ事象を$B_i$とし、白玉を選ぶ事象を$A$とする。
ここで白玉を引いたときにそれが袋B$_2$から引いたものである確率をベイズの定理を用いて計算する。
まず$B_i$だが3種類の袋を偏りなく選ぶため確率は$\frac{1}{3}$。ここで$P(A|B_1)=\frac{1}{3}$,$P(A|B_2)=\frac{1}{2}$,$P(A|B_3)=\frac{2}{5}$
よってベイズの定理より、
\begin{equation}
    P(B_2|A)=\frac{\frac{1}{3}\cdot\frac{1}{2}}{\frac{1}{3}\cdot\frac{1}{4}+\frac{1}{3}\cdot\frac{1}{2}+\frac{1}{3}\cdot\frac{2}{5}}=\frac{10}{23}\tag{1,24}
\end{equation}
\section{確率変数と確率分布}
\subsection{確率変数と確率分布}
\subsubsection{確率変数}
ある変数の値をとる確率が存在する変数を\textcolor{red}{確率変数}と呼ぶ。例えばサイコロならば1から6までの整数値をとるため1,2,3,4,5,6が確率変数となり、$P(X)=\frac{1}{6}$となる。
\subsubsection{離散型と連続型}
確率変数$X$が可算個の離散値の場合、確率変数$X$は\textcolor{red}{離散型}となる。また確率変数$X$が連続値をとる場合は\textcolor{red}{連続型}となる。以降\textcolor{red}{離散型か連続型によって使用できる式が異なる}ためしっかりと判別しよう。
\subsubsection{確率関数}
まず、\textcolor{blue}{離散型}確率変数の確率を表現する関数として次の式を考える。なお$X$は確率変数を、$x$は取りうる値を示している。今後の説明のため$x=x_1,x_2\cdots x_i\cdot$とする。
\begin{equation}
    f(x)=P(X=x)\tag{2,1}
\end{equation}
この$f(x)$を\textcolor{red}{確率関数}と呼ぶ。
ここで確率関数は次の2つの性質を当然満たす。
\begin{equation}
    f(x_i)>0\tag{2,2}
\end{equation}
\begin{equation}
    \sum^\infty_{i=1}f(x_i)= 1\tag{2,3}
\end{equation}
\subsubsection{確率密度関数}
次に\textcolor{blue}{連続型}の場合の確率関数のようなものを考える。
連続型確率変数$X$が区間$(a,b]$にあるとする。この時の確率を
\begin{equation}
    P(a<X\leq b)= \int_{a}^{b} f(t)dt\tag{2,4}
\end{equation}
で考える。この確率関数のようなものは次の2つの性質を当然満たす。
\begin{equation}
    f(x)\geq 0\tag{2,5}
\end{equation}
\begin{equation}
    \int_{-\infty}^{\infty} f(x)dx= 1\tag{2,6}
\end{equation}
この関数$f(x)$を\textcolor{red}{確率密度関数}と呼ぶ。
\subsubsection{分布関数}
確率関数および確率密度関数ではある1点やある区間での確率であった。しかし場合によっては確率が積もっていく様子を捉えたいこともある。
よって確率が積もっていく様子を捉える量として\textcolor{red}{分布関数}というものを用意する。分布関数は\textcolor{red}{大文字のFで表す。}分布関数は次で示す。
\begin{equation}
    F(X)=P(X\leq x)\tag{2,7}
\end{equation}
さらに分布関数は次の性質を持っている
\begin{itemize}
    \item $F(a)\leq F(b) \ \ \ (a<b)$
    \item $0\leq F(x) \leq 1$
    \item $F(x)$は右連続
\end{itemize}
さてここまで書いたことを考えると当然だが分布関数が決まると確率(密度)関数は同時に決定する。同様に確率(密度)関数が決定すると分布関数が決定する。
ここで\textcolor{blue}{離散型}の確率関数と分布関数の関係をまとめる。
\begin{equation}
    F(x) = P(X\leq x) =\sum_{x_i\leq x}f(x_i)\tag{2,8}
\end{equation}
次に\textcolor{blue}{連続型}の確率密度関数と分布関数の関係をまとめる。
\begin{equation}
    F(x) = P(X\leq x) = \int_{-\infty}^x f(t)dt\tag{2,9}
\end{equation}
なお密度関数や分布関数による確率的挙動を\textcolor{red}{確率分布}と呼んでいるが、確率分布については3節にて触れる。
\subsection{期待値と平均と分散}
\subsubsection{平均そして期待値}
\textcolor{blue}{離散型}の平均$\mu$について以下のように定義する。
\begin{equation}
    \mu = \sum_{i= 1,2,\cdots} x_if(x_i)\tag{2,10}
\end{equation}

実例を挙げてみよう。サイコロの出目の平均を計算すると3.5となる。

\textcolor{blue}{連続型}の平均$\mu$について以下のように定義する。なお確率変数$X$が取りうる領域を$\mathcal{X} $とする。
\begin{equation}
    \mu = \int_\mathcal{X}xf(x)dx\tag{2,11}
\end{equation}
そしてこれら平均を総じて確率変数$X$の\textcolor{red}{期待値}という。期待値は$E[x]$で表される。
なお$E[X]$は線形性を持っており場合によっては計算が楽である。
\subsubsection{分散}
確率変数のばらつきを捉えるものに\textcolor{red}{分散}がある。これは確率変数$X$の平均$\mu$からの離れ具合を2乗に基づいて平均的に測るものである。分散$\sigma^2=V(x)$は次式で定義する。
\begin{equation}
    \sigma^2=V[x]=E[(X-\mu)^2]\tag{2,12}
\end{equation}
これは\textcolor{blue}{離散型},\textcolor{blue}{連続型}どちらでも使うことができる。ただし期待値はそれぞれ対応する式で用いること。

\subsubsection{標準偏差}
分散はばらつきの尺度として優秀な一方で2乗しているため元の確率変数と単位が異なる。よって単位を合わせるために分散の平方根を\textcolor{red}{標準偏差}とする。
\subsubsection{k次モーメント}
一般的に$E[X^k]$を\textcolor{red}{k次モーメント}という。そして$E[(X-\mu)^k]$を\textcolor{red}{k次の中心モーメント}という。そして
今まで出たものをまとめると、平均は1次のモーメント、分散は2次の中心モーメントである。

ここで期待値が線形であり、$E[X]=\mu$を利用することで平均と分散に関して次の関係性を導くことができる。
\begin{eqnarray*}
    \sigma^2 &=& E[(X-\mu)^2]\\
    &=& E[X^2-2\mu X +\mu^2]\\
    &=& E[X^2]-2\mu E[X]+E[\mu^2]\\
    &=& E[X^2]-\mu^2
\end{eqnarray*}
\subsubsection{標準化}
世の中の無数の確率変数が同じ平均と標準偏差を持つように確率変数を変換することを\textcolor{red}{標準化}と呼ぶ。具体的に以下のように変換する。なお$Z$は確率変数である。
\begin{equation}
    Z=\frac{X-\mu}{\sigma}\tag{2,13}
\end{equation}
このとき平均$E[Z]$は0、分散$V[Z]$は1になる。

逆に標準化されている確率変数$Z$に対して、$X=\mu+\sigma Z$と変換すると、$E[X]=\mu$と$V[x]=\sigma^2$が簡単に得られる。

\subsection{多次元確率変数と同時確率分布と周辺確率分布}
\subsubsection{多次元確率変数}
まず1次元の確率変数$X_1,\cdots,X_k$をまとめた確率変数$X=(X_1,\cdots,X_k)$を\textcolor{red}{多次元確率変数}という。これはベクトルである。
\subsubsection{同時確率分布}
例えば袋の中に赤、青、黄の3色のボールが入ってることを考える。そしてそのボールには1から4までの数字が書かれている。
この時それぞれのボールの確率を求めるにはボールの色という確率変数とボールの数字という確率変数の2種類を考慮しなければならない。

このようにいくつかの確率変数を同時に扱うときの確率分布を\textcolor{red}{同時確率分布}という。

\textcolor{blue}{離散型}の場合、次式で示される。
\begin{equation}
    f(x_i,y_j\cdots)=P(X=x_i,Y=y_j\cdots)\tag{2,14}
\end{equation}
\textcolor{blue}{連続型}の場合、次式で示される。
\begin{equation}
    f(a<X\leq b,c<Y\leq d\cdots)=\int_a^b\int_c^d\cdot f(x,y,\cdots) dxdy\cdots\tag{2,15}
\end{equation}
\subsubsection{周辺確率分布}
\textcolor{red}{周辺確率}とは、ただ1つだけの事象が起こる確率である。
つまり1つの変数を固定し、もう1つの変数のパターンをすべて足せばいい。
\textcolor{blue}{離散型}の場合、次式を例とする。
\begin{equation}
    P(X_1=x_1)=\sum_{X_2}\sum_{X_3}\cdots P(X_1 = x_1,X_2 = x_2, X_3 = x_3\cdots)\tag{2,16}
\end{equation}
要は1つの確率変数を固定して後ひたすら足しているだけなのだ。
\textcolor{blue}{連続型}の場合、次式を例とする。
\begin{equation}
    P(X_1\leq x_1,-\infty < X_2 < \infty,-\infty < X_3 < \infty \cdots) = \int_{-\infty}^x\int_{-\infty}^\infty\int_{-\infty}^\infty\cdots f(x_2,x_3,\cdots)dx_2dx_3\cdots\tag{2,17}
\end{equation}
\subsection{共分散}
2つの確率変数$X$と$Y$の関係を表す量として\textcolor{red}{共分散}を次式で定義する。
\begin{equation}
    \sigma_{xy}=Cov[X,Y]=E[(X-\mu_x)(Y-\mu_y)] \tag{2,18}
\end{equation}
また共分散の代わりに\textcolor{red}{相関係数}に着目することもある。相関係数は次式で定義される。
\begin{equation}
    \rho_{xy} = Corr[X,Y] = \frac{Cov[X,Y]}{\sqrt{V[X]V[Y]}}\tag{2,19}
\end{equation}
\subsection{確率変数の和の平均と分散}
確率変数の和$X+Y$における平均と分散は次式で表すことができる。
\begin{itemize}
    \item 和：$E[X+Y] = E[X]+E[Y]$
    \item 分散: $V[X+Y] = V[X]+V[Y] + 2Cov[X,Y]$
\end{itemize}
なお分散については以下に導出を記載しておく。
\begin{eqnarray*}
    V[X+Y] &=& E[\{(X+Y)-(\mu_x+\mu_Y)\}^2]\\
            &=& E[\{(X-\mu_x)+(Y-\mu_Y)\}^2]\\
            &=& E[(X-\mu_x)^2]+E[(Y-\mu_y)^2]+2E[(X-\mu_x)(Y-\mu_y)]\\
            &=& V[X]+V[Y]+2Cov[X,Y]
\end{eqnarray*}
\subsection{確率不等式}
\subsubsection{チェビシェフの不等式}
\textcolor{red}{チェビシェフの不等式}を紹介する。
確率変数$X$における平均と分散を用いる。このとき任意の$\varepsilon >0$に対して、次が成り立つ。
\begin{equation}
    P[(|X-\mu|\geq\varepsilon)]\leq \frac{\sigma^2}{\varepsilon^2}\tag{2,20}
\end{equation}
この不等式は、特定の値以下もしくは以上の累積分布関数の最大値を求めることができる。
\subsubsection{コーシー・シュバルツの不等式}
次に\textcolor{red}{コーシー・シュバルツの不等式}を紹介する。
コーシー・シュバルツの不等式は確率変数$X$と$Y$の分散と共分散との間に次の不等式が成立するというものである。
\begin{equation}
    Cov[X,Y] ^2,\leq V[X]V[Y]\tag{2,21}
\end{equation}
\subsection{条件付密度関数}
確率変数$X=x$、$Y=y$の時の\textcolor{red}{条件付密度関数}$f_{Y|X}(y|x)$を次式で定義する。これは確率変数が2つの$f_{(X,Y)}(x,y)$で$X$が起きた時に$Y$が発生する確率である。
\begin{equation}
    f_{Y|X}(y|x)=\frac{f_{(X,Y)}(x,y)}{f_X(x)}\tag{2,22}
\end{equation}
なおここでの$f_X(x)$は周辺確率分布(2.3.3を参照)である。
\section{確率分布}
\subsection{\textcolor{blue}{離散型}確率分布}
\subsubsection{一様分布}
確率変数の値に関わらず一定の確率をとるような確率モデルの場合、\textcolor{blue}{離散型}確率変数$X$は\textcolor{red}{一様分布}に従うといわれる。
\begin{equation}
    f(x_i) = P(X=x_i) = \frac{1}{n}\ \ \ \ \ \ \ \ (i=1,\cdots ,n)\tag{3,1}
\end{equation}

実例を見てみよう。サイコロを振ったときに出る目の確率を考える。確率変数$X$は1から6までの値をとる。そして確率変数にかかわらず確率は常に$\frac{1}{6}$となる。

一様分布における確率分布の平均と分散を考える。なお標本空間$\Omega={1,2,\cdots,n}$とする。
まず期待値だが
\begin{equation}
    E[X] = 1*p+2*p+\cdots n*p = \frac{n(n+1)}{2}p = \frac{n+1}{2}\tag{3,2}
\end{equation}
次に分散だが、
\begin{eqnarray*}
    V[X] &=& E[X^2]-(E[X])^2\\
    &=& \frac{n(n+1)(2n+1)}{6}p-(E[n])^2\\
    &=& \frac{2n^2+3n+1}{6}-\frac{n^2+2n+1}{4}\\
    &=& \frac{4n^2+6n+2}{12}-\frac{3n^2+6n+3}{12}\\
    &=&\frac{n^2-1}{12}
\end{eqnarray*}
よって
\begin{equation}
    V[X] = \frac{n^2-1}{12}\tag{3,3}
\end{equation}

\subsubsection{ベルヌーイ分布}
まず確率変数が2つの値0と1しかとらないという状況を考える。
ここで$P(1)=\theta$と定義すると、$P(0)=1-\theta$となる。もちろん$0<\theta < 1$となる。
このときこの確率は次のように表現できる。
\begin{equation}
    P(x) = \theta^x (1-\theta)^{1-x}\tag{3,4}
\end{equation}
この密度関数をもつ\textcolor{blue}{離散型}確率変数$x$は、\textcolor{red}{ベルヌーイ分布}に従うという。

実例を考えてみよう。10\%の確率で当選するくじがあったとする。くじに当選する事象を$X=1$、外れる事象を$X=0$とすると、$\theta = 0.1$のため
\begin{equation*}
    P(x) = 0.1^x(0.9)^{1-x}
\end{equation*}
となる。

ベルヌーイ試行における平均と分散を求める。まず標本空間は0と1の2種類である。
まず期待値だが、
\begin{equation}
    E[x] = \theta\tag{3,5}
\end{equation}
次に分散だが、
\begin{equation}
    V[x] = \theta -\theta^2= \theta(1-\theta)\tag{3,6}
\end{equation}
ここは比較的簡単に計算できる。
\subsubsection{2項分布}
ここでベルヌーイ分布をとるような試行、つまりベルヌーイ試行を複数回繰り返すような状況を考える。
互いに独立したベルヌ―イ試行を$n$回繰り返したとき、ベルヌーイ試行の確率変数$X$が1となるような状況が$y$回起こった時を考える。
$Y$はもちろん確率変数となる。この時、この事象の組み合わせは$nCy = \frac{n!}{(n-y)!y!}$であり、
それぞれが起きる確率は$\theta^y (1-\theta)^{n-y}$となるため、
\begin{equation}
    f(y) = P(Y=y)= \frac{n!}{(n-y)!y!}\theta^y (1-\theta)^{n-y}\ \ \ \ \ \ \ (y=0,1,\cdots, n)\tag{3,7}
\end{equation}
さて2項分布における期待値と分散を計算する。
なおこの計算は非常に長いため最悪結果だけ見ること。
まず標本空間は$N$回の施行で$2^N$回存在する。
さてこの場合の平均を考える。
\begin{eqnarray*}
    E[X]&=&\sum_{k=0}^n k\times \frac{n!}{(n-k)!k!}\theta^k (1-\theta)^{n-k}\\
    &=& \sum_{k=1}^n \frac{n!}{(n-k)!(k-1)!}\theta^k (1-\theta)^{n-k}\\
    &=& \sum_{k=1}^n n\frac{(n-1)!}{(n-k)!\{(n-1)-(n-k)\}!}\theta\cdot\theta^{n-1}(1-\theta)^{\{(n-1)-(k-1)\}}\\
    &=& n\theta\sum_{k=1}^n \frac{(n-1)!}{(n-k)!\{(n-1)-(n-k)\}!}\theta^{n-1}(1-\theta)^{\{(n-1)-(k-1)\}}\\
    &=& n\theta\{(\theta+(1-\theta))\}^{n-1}\\
    &=& n\theta
\end{eqnarray*}
よって
\begin{equation}
    E[x] = n\theta\tag{3,8}
\end{equation}
後述するモーメント母関数を使用することが簡単である。

次に分散を考える。
分散は次のように変形する。
\begin{eqnarray*}
    V[X]&=&E[X^2]-(E[X])^2\\
\end{eqnarray*}
ここで、
\begin{eqnarray*}
    E[X^2] &=& \sum_{k=0}^n k^2\times \frac{n!}{(n-k)!k!}\theta^k (1-\theta)^{n-k}\\
    &=& \sum_{k=0}^n k(k-1)\frac{n!}{(n-k)!k!}\theta^k (1-\theta)^{n-k}+\sum_{k=0}^n k\frac{n!}{(n-k)!k!}\theta^k (1-\theta)^{n-k}\\
    &=& \sum_{k=0}^n n\cdot(n-1)\frac{n!}{(k-2)!\{(n-2)-(k-2)\}!}\theta^2\cdot \theta^{n-2}(1-\theta)^{\{(n-2)-(k-2)\}}+n\theta\\
    &=& n(n-1)\theta^2+n\theta\\
    &=& n^2\theta^2-n\theta^2+n\theta\\
    &=& n^2\theta^2+n\theta(1-\theta)\\
\end{eqnarray*}
よって
\begin{equation}
    V[X] = n^2\theta^2+n\theta(1-\theta)-n^2\theta^2 = n\theta(1-\theta)\tag{3,9}
\end{equation}
もちろんこれもモーメント母関数を使用する方が簡単である。
\subsubsection{ポアソン分布}
二項分布をさらに発展させて考えてみる。ベルヌーイ試行の確率変数$X$が1となる確率がとても小さい状況。しかしながら試行回数$n$は膨大であるという状況を考える。この時$y$回確率変数$X=1$となる試行が発生する確率を求める。

確率変数$X=1$となる回数が$Y$となるときの期待値$E[Y] = n\theta = \lambda$とする。
つまり
\begin{equation}
    0<n\theta = \lambda < \infty \ \ \ \ \ \ (n->\infty,\theta->0)\tag{3,4}
\end{equation}
この時の2項分布の密度関数の極限として次が求めることができる。
\begin{equation}
    f(Y) = P(Y = y)=\frac{\lambda^y}{y!}e^{-\lambda}\tag{3,10}
\end{equation}
この密度関数を持つ\textcolor{blue}{離散型}確率変数$Y$は、\textcolor{red}{ポアソン分布}に従うといわれる。

ポアソン分布においても平均と分散を考える。
まず平均を求める。なお計算についてはマクローリン展開を用いてることを注意してほしい。
\begin{eqnarray*}
    E[X] &=& \sum_{y=0}^\infty y\frac{\lambda^y}{y!}e^{-\lambda}\\
        &=&\sum_{y=0}^\infty \lambda e^{-\lambda}\frac{\lambda^{y-1}}{(y-1)!}\\
        &=& \lambda e^{-\lambda}\sum_{y=0}^\infty \frac{\lambda^{y-1}}{(y-1)!}\\
        &=& \lambda e^{-\lambda}e^{\lambda}\\
        &=& \lambda
\end{eqnarray*}
この結果より
\begin{equation}
    E[X] = \lambda \tag{3,11}
\end{equation}
次に分散を求める。
\begin{eqnarray*}
    V[X] &=& E[X^2]-(E[x])^2
\end{eqnarray*}
ここで、
\begin{eqnarray*}
    E[X^2] &=& \sum_{y=0}^\infty y(y-1)\frac{\lambda^y}{y!}e^{-\lambda}+\sum_{y=0}^\infty y\frac{\lambda^y}{y!}e^{-\lambda}\\
        &=&\sum_{y=0}^\infty \lambda^2 e^{-\lambda}\frac{\lambda^{y-2}}{(y-2)!}+\lambda\\
        &=& \lambda^2 e^{-\lambda}\sum_{y=0}^\infty \frac{\lambda^{y-2}}{(y-2)!}+\lambda\\
        &=& \lambda^2 e^{-\lambda}e^{\lambda}+\lambda\\
        &=& \lambda^2+\lambda
\end{eqnarray*}
よって、
\begin{equation}
    V[X]  = \lambda^2+\lambda-\lambda^2=\lambda\tag{3,12}
\end{equation}

\subsubsection{幾何分布}
ここで一度話をベルヌーイ試行まで戻す。初めて$X=1$となるまでの試行回数$Y$の確率分布を\textcolor{red}{幾何分布}と呼ぶ。このとき次式で密度関数は示される。
\begin{equation}
    P(Y=y) = (1-\theta)^{y-1}\theta\ \ \ \ \ \ \ (k=1,2,\cdots)\tag{3,13}
\end{equation}
\subsection{\textcolor{blue}{連続型}確率分布}
\subsubsection{一様分布}
\textcolor{blue}{連続型}確率変数$X$が区間$(a,b)$の間で、一様にどれかの値をとる可能性があるとき、
\begin{equation}
    f(x)=\frac{1}{b-a}\tag{3,14}
\end{equation}
この密度関数を持つ連続型確率変数$X$は区間$(a,b)$上の一様分布に従うといわれる。これを$X\sim U(a,b)$と表記する。
なお期待値は、
\begin{equation}
    E(X)=\frac{b+a}{2}\tag{3,15}
\end{equation}
分散は、
\begin{equation}
    V(X)=\frac{(b-a)^2}{12}\tag{3,16}
\end{equation}
となる。
導出を以下に示す。
\begin{eqnarray*}
    E(X) &=& \int_a^b \frac{1}{b-a}xdx\\
        &=& \left[\frac{x^2}{2(b-a)}\right]_a^b\\
        &=& \frac{(b+a)(b-a)}{2(b-a)}\\
        &=& \frac{b+a}{2}
\end{eqnarray*}
\begin{eqnarray*}
    E(X^2) &=& \int_a^b \frac{1}{b-a}x^2dx\\
    &=& \left[\frac{x^3}{3(b-a)}\right]_a^b\\
    &=& \frac{(b^3-a^3)}{3(b-a)}\\
    &=& \frac{b^2+ab+a^2}{3}
\end{eqnarray*}
\begin{eqnarray*}
    V(X) &=& \frac{b^2+ab+a^2}{3}-\frac{a^2+2ab+b^2}{4}\\
    &=& \frac{b^2-2ab+a^2}{12}\\
    &=& \frac{(b-a)^2}{12}
\end{eqnarray*}
\subsubsection{指数分布}
ある期間に平均して$\lambda$回起こる現象が、次起こるまでの期間$X$が指数分布に従うとき、$X=x$となる確率密度関数は次式で示される。
\begin{equation}
    f(x) = \lambda e^{-\lambda x}\tag{3,17}
\end{equation}
この時この\textcolor{blue}{連続型}確率分布を\textcolor{red}{指数分布}と呼ぶ。そしてこの状況を$X\sim Ex(\lambda)$と表記する。
なお期待値と分散は次のようになります。
\begin{equation}
    E(X)=\frac{1}{\lambda}\tag{3,18}
\end{equation}
\begin{equation}
    V(X)=\frac{1}{\lambda^2}\tag{3,19}
\end{equation}
これも導出を以下に示す。
\begin{eqnarray*}
    E(X) &=& \int_0^\infty x\lambda e^{-\lambda x}dx \\
    &=& \left[x(-e^{-\lambda x})\right]_0^\infty -\int_0^\infty(-e^{-\lambda x})dx\\
    &=& 0-0+\left[-\frac{e^{-\lambda x}}{\lambda}\right]_0^\infty\\
    &=& 0-(-\frac{1}{\lambda})\\
    &=& \frac{1}{\lambda}
\end{eqnarray*}
\begin{eqnarray*}
    E(X^2)&=& \int_0^\infty x^2\lambda e^{-\lambda x}dx \\
    &=& \left[x^2(-e^{-\lambda x})\right]_0^\infty -2\int_0^\infty x(-e^{-\lambda x})dx\\
    &=& 0+\frac{2}{\lambda^2}\\
    &=& \frac{2}{\lambda^2}\\
\end{eqnarray*}
\begin{eqnarray*}
    V(X) &=& \frac{2}{\lambda^2} - \frac{1}{\lambda^2}\\
    &=& \frac{1}{\lambda^2}
\end{eqnarray*}
\subsubsection{正規分布}
確率変数$X$が次の密度関数を持つとき、\textcolor{red}{正規分布}に従うと言われ$X\sim N(\mu,\sigma^2)$とあらわされる。
\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\tag{3,20}
\end{equation}
急に式が出てきて混乱しかねないこの正規分布はどんな特徴があるの以下に示す。
\begin{itemize}
    \item 平均値/最頻値/中央値が一致する。
    \item 平均値を中心に左右対称である。
    \item x軸が漸近線となる。
    \item 分散が大きくなると、曲線の山は低くなり、左右に広がって平らになる。
\end{itemize}
ここで正規分布の平均と分散を求める。
まず期待値だが、
\begin{eqnarray*}
    E[X] &=& \int_{-\infty}^{\infty} x\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=& \int_{-\infty}^{\infty} (x-\mu+\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=& \int_{-\infty}^{\infty} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx+\int_{-\infty}^{\infty} \mu\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=& E[X-\mu] +E[\mu]\\
\end{eqnarray*}
ここで書いた$ E[X-\mu] +E[\mu]$は平均に線形の性質があることを考えたら当然ではある。

ここで$E[X-\mu]$を考える。正規分布の特徴として$x=\mu$において左右対称であることを考えたら、
\begin{eqnarray*}
    E[X-\mu]&=& \int_{-\infty}^{\infty} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=& \int_{-\infty}^{0} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx+\int_{0}^{\infty} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=&\int_{-\infty}^{0} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx-\int_{-\infty}^{0} (x-\mu)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\
    &=& 0
\end{eqnarray*}
また$E[\mu]$を考える。$E[\mu]$は線形性より、$E[\mu-X]+E[X]$。そして前の計算を基にすると、$E[X]=E[\mu$]

さて気づいただろうか。$E[x]$は最初に平均を$\mu$と定義しているため$E[X]=\mu$ということが自明である。論理もくそもない。今までの記述は全部無駄だったということである。

同様に分散も(以下略)。

冗談は置いといて期待値で示した奇関数の関係は理解しておいて損はない。

以降で使用するが、平均$\mu=0$、分散$\sigma^2=1$である$N(0,1)$の正規分布を\textcolor{red}{標準正規分布}と呼ぶ。
\subsubsection{ガンマ分布}
指数分布を一般化させた分布。ある期間$\beta$ごとに平均して1回起こる現象が$\alpha$回起きるまでの期間$X$がガンマ分布に従うときに、ガンマ分布は次式で示される。ただし$\alpha,\beta>0$
\begin{equation}
    f(X) = \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha-1}e^{-\frac{x}{\beta}}\tag{3,21}
\end{equation}
ただし$\Gamma$関数は次式で定義する。
\begin{equation}
    \Gamma(\alpha) = \int^\infty_0x^{\alpha-1}e^{-x}dx\tag{3,22}
\end{equation}
ここで$\alpha\in\mathbb{N}$の時の$\Gamma$関数は、
\begin{equation}
    \Gamma(\alpha) = (\alpha-1)!\tag{3,23}
\end{equation}
この関数の平均と分散は次式になる。
\begin{equation}
    E[X] = \frac{\alpha}{\beta}\tag{3,24}
\end{equation}
\begin{equation}
    V[X] = \frac{\alpha}{\beta^2}\tag{3,25}
\end{equation}
ここでお気づきかはわからないが、$\alpha=1$のときに、$\lambda= \frac{1}{\beta}$とすると、指数分布と同じになる。ある期間に$\lambda$回起こると期間$\beta$に1回起こるとの関係を考えると当たり前の話ではある。
\subsubsection{カイ2乗分布}
確率変数$X_1,X_2,\cdots,X_n$が独立に標準正規分布に従うとする。ここで確率変数$X$を、$X=X_1^2+X_2^2+\cdots+X_n^2$とすると、
\subsection{モーメント母関数}
確率分布の性質を表現する手法として\textcolor{red}{モーメント母関数}$\psi(x)$を新たに定義する。
\begin{equation}
    \psi(x) = E[e^{tX}] = \int e^{tx}f(x)dx\tag{3,21}
\end{equation}
なお$t$はパラメータである。
モーメント母関数を利用することで、多くの分布の平均や分散を比較的簡単に求めることができる。
$t$で1階微分したときの$t=0$の値が期待値を、2階微分することで分散を求めるのに必要な2次モーメントを求めることができる。

さて実際に求めてみましょう。
\subsubsection{\textcolor{blue}{離散型}一様分布}
$e^{tx}$が等比数列であることに注目して、
\begin{eqnarray*}
    \psi(x) &=& E[e^{tX}] \\
    &=& \sum_{x=1}^n e^{tx}\frac{1}{n}\\
    &=& \frac{1}{n}\sum_{x=1}^n e^{tx}\\
    &=& \frac{1}{n}\frac{e^t(1-e^{tn})}{(1-e^t)}\\
    &=& \frac{e^t(1-e^{tn})}{n(1-e^t)}
\end{eqnarray*}
よって、
\begin{equation}
    \psi(x) = \frac{e^t(1-e^{tn})}{n(1-e^t)}\tag{3,22}
\end{equation}
なお一様分布関数のモーメント母関数を利用した計算はロピタルの定理等が必要でありあまり恩恵を感じることができないため省略する。
\subsubsection{ベルヌーイ分布}
\begin{eqnarray*}
    \psi(x) &=& E[e^{tX}] \\
    &=& \sum_{x=0}^1 e^{tx}\theta^x (1-\theta)^{1-x}\\
    &=&(1-\theta)+e^t\theta
\end{eqnarray*}
よって
\begin{equation}
    \psi(x) = (1-\theta)+e^t\theta\tag{3,23}
\end{equation}
次に平均を求める。
\begin{equation}
    E[X] = 0+\theta e^t\rightarrow \theta(t\rightarrow 0)\tag{3,24}
\end{equation}
最後に分散を求める。
\begin{equation}
    E[X^2] = 0+\theta e^t\rightarrow \theta(t\rightarrow 0)\tag{3,25}
\end{equation}
\begin{equation}
    V[X] = E[X^2]-(E[X])^2 = \theta (1-\theta)\tag{3,26}
\end{equation}
\subsubsection{二項分布}
まずモーメント母関数を求める。２項定理を用いて、
\begin{eqnarray*}
    \psi(x) &=& E[e^{tX}]\\
    &=& \sum_{x=0}{n} e^{tx}\frac{n!}{x!(n-X)!}p^x(1-p)n-x\\
    &=&\sum_{x=0}{n} \frac{n!}{x!(n-X)!}(pe^t)^x(1-p)n-x\\
    &=& (pe^t + 1 - p)^n
\end{eqnarray*}
よってモーメント母関数は、
\begin{equation}
    \psi(x) =(pe^t + 1 - p)^n\tag{3,27}
\end{equation}
ここでモーメント母関数を用いて平均を求める。
\begin{eqnarray*}
    E[X] &=& \frac{d\psi(x)}{dt}\\
    &=& n(p^et + 1 -p)^{n-1}pe^t\\
    &\rightarrow& np(t\rightarrow 0)
\end{eqnarray*}
この結果より
\begin{equation}
    E[x] = np \tag{3,28}
\end{equation}
次に分散を求める。ために$E[X^2]$を求める。
\begin{eqnarray*}
    E[x^2] &=& \frac{d^2\psi(x)}{dt^2}\\
           &=& n(n-1)(pe^t+1-p)^{n-2}p^2e^{2t}+n(p^et + 1 -p)^{n-1}pe^t\\
           &\rightarrow& n(n-1)p^2+np(t\rightarrow 0)\\
           &=& n^2p^2-np^2+np
\end{eqnarray*}
よって、
\begin{equation}
    V[x]= n^2p^2-np^2+np -n^2p^2=np(1-p)\tag{3,29}
\end{equation}
このようにモーメント母関数さえ求めることができれば平均や分散は簡単に求めることができる。
\subsubsection{ポアソン分布}
ポアソン分布におけるモーメント母関数を求める。
マクローリン展開を用いて求める。
\begin{eqnarray*}
    \psi(x) &=& \sum_{y=0}^\infty e^{tx}\frac{\lambda^x}{x!}e^{-\lambda}\\
    &=& e^{-\lambda}\sum_{y=0}^\infty e^{tx}\frac{\lambda^x}{x!}\\
    &=& e^{-\lambda} e^{e^t\lambda}\\
    &=& e^{\lambda(e^t-1)}
\end{eqnarray*}
よって、
\begin{equation}
    \psi(x) = e^{\lambda(e^t-1)}\tag{3,30}
\end{equation}
この結果をもとに期待値と分散を求める。
\begin{eqnarray*}
    E[X] &=& \frac{d\psi(x)}{dt}\\
    &=&  \lambda e^t e^{\lambda(e^t-1)}\\
    &\rightarrow& \lambda(t\rightarrow 0)
\end{eqnarray*}
よって、
\begin{equation}
    E[x] = \lambda \tag{3,31}
\end{equation}
分散のための$E[x^2]$を求める。
\begin{eqnarray*}
    E[x^2] &=& \frac{d^2\psi(x)}{dt^2}\\
        &=& \lambda e^\lambda(e^t-1)e^t + \lambda^2e^te^\lambda(e^t-1)\\
        &\rightarrow& \lambda^2 + \lambda (t\rightarrow 0)
\end{eqnarray*}
よって分散は
\begin{equation}
    V[x]=\lambda^2+\lambda -\lambda^2 =\lambda \tag{3,32}
\end{equation}
\subsubsection{\textcolor{blue}{連続型}一様分布}

モーメント母関数は次の通りである。
\begin{eqnarray*}
    \psi(x) &=& \int_a^b \frac{1}{b-a}e^{tx}dx\\
    &=& \left[\frac{e^tx}{t(b-a)}\right]_a^b\\
    &=& \frac{e^{tb}-e^{ta}}{t(b-a)}
\end{eqnarray*}
よって
\begin{equation}
    \psi(x) = \frac{e^{tb}-e^{ta}}{t(b-a)}\tag{3,31}
\end{equation}
次に平均と分散だがロピタルの定理を使うためめんどくさいため省略する。
\subsubsection{指数分布}

指数分布におけるモーメント母関数を求める。なお$t-\lambda <0$ということとする。($t\geq 0$では母関数モーメントは存在しない)
\begin{eqnarray*}
    \psi(x) &=&\int_0^\infty e^{tx}\lambda e^{-\lambda x}\\
    &=& \lambda\left[\frac{1}{t-\lambda}e^{(t-\lambda)x}\right]\\
    &=& \frac{\lambda}{t-\lambda}(0-1)\\
    &=& \frac{\lambda}{\lambda - t}
\end{eqnarray*}
よって、
\begin{equation}
    \psi(x)  = \frac{\lambda}{\lambda - t}\tag{3,32}
\end{equation}
ここから平均求める
\begin{eqnarray*}
    E[X] &=& \frac{d\psi(x)}{dt}\\
    &=& \frac{\lambda}{(\lambda-t)^2}\\
    &\rightarrow&\frac{1}{\lambda}(t\rightarrow 0)\\
\end{eqnarray*}
よって
\begin{equation}
    E[X] = \frac{1}{\lambda}\tag{3,33}
\end{equation}
次に分散を求める。
\begin{eqnarray*}
    E[x^2] &=& \frac{d^2\psi(x)}{dt^2}\\
    &=& \frac{2\lambda(\lambda-t)}{(\lambda-t)^4}\\
    &=& \frac{2\lambda}{(\lambda-t)^3}\\
    &\rightarrow&\frac{2}{\lambda^2}(t\rightarrow 0)
\end{eqnarray*}
よって、
\begin{equation}
    V[X] = \frac{2}{\lambda^2}-\frac{1}{\lambda^2} =\frac{1}{\lambda^2}\tag{3,33}
\end{equation}
\subsubsection{正規分布}
正規分布におけるモーメント母関数を求める。
\begin{eqnarray*}
    \psi(x) &=& \int_{-\infty}^\infty e^{xt}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
        &=& \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2-2x\mu+\mu^2-2\sigma^2xt}{2\sigma^2}}\\
        &=& \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2+\mu^2+2\mu\sigma^2t+\sigma^4t^2-2x\mu-2x\sigma^2t-2\mu\sigma^2t-\sigma^4t^2}{2\sigma^2}}\\
        &=& \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-(\mu+\sigma^2t))}{2\sigma^2}+\mu t +\frac{\sigma^2t^2}{2}}\\
        &=& e^{\mu t+\frac{\sigma^2t^2}{2}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-(\mu+\sigma^2t))}{2\sigma^2}}
\end{eqnarray*}
ここで$\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-(\mu+\sigma^2t))}{2\sigma^2}}$は平均を$\mu+\sigma^2t$とみたてた新たな正規分布における。全標本空間の累積分布関数であるとする。
このとき全確率は1であるため、
\begin{equation}
    \psi(x) = e^{\mu t+\frac{\sigma^2t^2}{2}}\tag{3,34}
\end{equation}
これを用いて期待値と分散を求める。
\begin{eqnarray*}
    E[X] &=& \frac{d\psi(x)}{dt}\\
    &=& (\mu+\sigma^2t)e^{\mu t+\frac{\sigma^2t^2}{2}}\\
    &\rightarrow&(\mu+0)\times 1(t\rightarrow 0)\\
    &=& \mu
\end{eqnarray*}
よって、
\begin{equation}
    E[X] = \mu\tag{3,35}
\end{equation}
分散は、
\begin{eqnarray*}
    E[x^2] &=& \frac{d^2\psi(x)}{dt^2}\\
    &=& (\sigma^2)e^{\mu t+\frac{\sigma^2t^2}{2}}+(\mu+\sigma^2t)^2e^{\mu t+\frac{\sigma^2t^2}{2}}\\
    &\rightarrow& \sigma^2+\mu^2(t\rightarrow 0)
\end{eqnarray*}
よって、
\begin{equation}
    V[X] = \sigma^2 + \mu^2 - \mu^2 = \sigma^2 \tag{3,36}
\end{equation}
\section{中心極限定理}
\subsection{大数の定理}
母平均$\mu$である集団から標本を抽出する場合、試行回数が大きくなるについて、標本平均は母平均$\mu$に近づくという定理を\textcolor{red}{大数の定理}と呼ぶ。
これは確率分布がいかなるものであっても必ず成立する。
\subsection{中心極限定理}
考える確率分布が正規分布に従う場合でも従わない場合でも試行回数が大きくなるにつれて標本平均の分布は平均$\mu$,分散$\frac{\sigma^2}{n}$の正規分布$N(\mu,\frac{\sigma^2}{n})$という定理を\textcolor{red}{中心極限定理}という。


\end{document}